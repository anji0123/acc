My project full end to end
Data sources - API
Sql server on prem
azure sql data base
Csv files and flat files
Storage -	blob -  heriracy enable - adls
Subscription -	 resource manager - storage - container - retail - bronze - silver - gold 

 
 
â€œWeâ€™re a 5-member team focused on retail banking pipelines â€” UPI, ATM, POS, and customer transactions. We ingest from SQL Server, MySQL, and flat files dropped by upstream systems. Other domains like risk and compliance have separate teams, but we collaborate closely on shared infrastructure and governance.â€
Our 5-member team in retail banking owns ingestion from digital channels like ATM, UPI, and POS, as well as payment gateways and branch systems. We use ADF and Databricks for real-time and batch ETL. For core banking and CRM systems, we collaborate with upstream teams who provide curated views or files. We also monitor pipeline health using Azure Monitor and Power BI logs.â€
1. Digital Banking Channels
â€¢	Examples: Mobile apps, online banking portals, ATM networks
â€¢	Data: UPI, NEFT, ACH, fund transfers, login events
â€¢	Ingestion:
o	Real-time via Event Grid triggers
o	Databricks Structured Streaming for ATM/UPI logs
o	ADF pipelines for batch channel summaries
â€¢	Why you own it: These are core to retail banking KPIs and dashboards.
2. Payment Gateways & Card Networks
â€¢	Examples: Visa, Mastercard, RuPay, POS terminals
â€¢	Data: Card swipes, failed payments, fraud alerts
â€¢	Ingestion:
o	Batch via SFTP flat files
o	Real-time via Azure Functions + Event Grid
â€¢	Why you own it: These feed merchant spend dashboards and fraud detection logic.
3. Branch & Teller Systems
â€¢	Data: In-branch deposits, withdrawals, check processing
â€¢	Ingestion:
o	Batch via SQL Server extracts
o	CSV files dropped into ADLS Bronze layer
â€¢	Why you own it: These are part of daily retail transaction volumes.
4. Cloud-Native Sources
â€¢	Examples: Azure Monitor, Power BI usage logs
â€¢	Data: Pipeline metrics, dashboard refresh stats
â€¢	Ingestion:
o	Direct integration with Log Analytics, Kusto, Power BI REST API
â€¢	Why you own it: Helps monitor your own pipeline health and dashboard SLAs.
ğŸŸ¡ ğŸ”„ Partially Owned / Collaborated With
ğŸ¯ Summary: What Your Team Owns
Ownership Level	Data Source Category
âœ… Fully Owned	Digital Channels, Payment Gateways, Branch Systems, Cloud Logs
ğŸ”„ Partially Owned	Core Banking Systems, CRM, Call Center Logs
âŒ Not Owned	Third-Party Providers, Internal Ops Systems
ğŸ§  Interview-Ready Answer
â€œOur 5-member team in retail banking owns ingestion from digital channels like ATM, UPI, and POS, as well as payment gateways and branch systems. We use ADF and Databricks for rea
What Happens After Getting the Data in Retail Banking ETL
Once your team ingests data from sources like ATM logs, UPI transactions, POS swipes, and branch systems, the next steps are:
âœ… 1. Data Cleaning & Standardization
â€¢	Handled by: Your team (ETL engineers)
â€¢	Tools: PySpark in Databricks, ADF Data Flows
â€¢	Tasks:
o	Remove duplicates
o	Handle nulls
o	Standardize formats (e.g., date, currency, channel codes)
o	Apply business rules (e.g., flag suspicious transactions)
âœ… 2. Data Modeling (Dimensional Design)
â€¢	Handled by: Often a mix of:
o	Your team (if youâ€™re full-stack data engineers)
o	BI/reporting team (if they own dashboard logic)
o	Data architects (for enterprise-wide consistency)
ğŸ”¹ What You Might Design:
â€¢	Fact tables: fact_transactions, fact_customer_activity, fact_card_swipes
â€¢	Dimension tables: dim_customer, dim_merchant, dim_channel, dim_time
â€¢	SCD logic: For slowly changing dimensions like customer address
ğŸ”¹ Tools Used:
â€¢	Synapse SQL
â€¢	Databricks Delta Lake
â€¢	ERD tools (Lucidchart, dbt, or internal modeling tools)
âœ… 3. Data Enrichment
â€¢	Handled by: Your team or upstream teams
â€¢	Examples:
o	Join with merchant metadata
o	Add fraud flags
o	Map channel codes to readable names
âœ… 4. Data Loading into Warehouse
â€¢	Handled by: Your team
â€¢	Destination: Azure Synapse (Gold layer)
â€¢	Purpose: Power BI dashboards, executive reporting, analytics
âœ… 5. Governance & Access Control
â€¢	Handled by: Data governance team + your team
â€¢	Tasks:
o	Mask PII
o	Apply RBAC
o	Track lineage
o	Audit logs
âœ… 6. Dashboard Consumption
â€¢	Handled by: BI/reporting team
â€¢	Tools: Power BI, Tableau
â€¢	Data Source: Synapse views or import mode from Gold layer
ğŸ§  Interview-Ready Answer
â€œAfter ingesting data from retail banking sources like ATM, UPI, and POS, our team cleans and standardizes it using PySpark and ADF. We design dimensional models â€” fact and dimension tables â€” in Synapse, often collaborating with BI and data architecture teams. We enrich the data, apply SCD logic, and load it into the Gold layer for Power BI dashboards. Governance teams help with masking and access control, ensuring compliance and audit readiness.â€
â€¢	ETL + modeling for Synapse or Delta Lake
â€¢	Designing fact tables for transactions, customer activity, merchant spend
â€¢	Creating dimension tables for time, channel, customer, merchant
â€¢	Implementing SCD Type 2 for slowly changing dimensions
ğŸ§© What Youâ€™re Expected to Model
In your retail banking context, youâ€™d likely design:
Table Type	Examples
Fact Tables	fact_transactions, fact_card_swipes, fact_customer_activity
Dimension Tables	dim_customer, dim_channel, dim_merchant, dim_time
Bridge Tables	For many-to-many relationships (e.g., customer-product)
Audit Tables	etl_audit_log, pipeline_control_table
Your Data Modeling Responsibilities (Mapped to Owned Sources)
âœ… 1. Digital Banking Channels
ğŸ”¹ Data: UPI, NEFT, ACH, fund transfers, login events
ğŸ”¹ Modeling Tasks:
â€¢	Design fact tables like fact_digital_transactions, fact_login_activity
â€¢	Create dimension tables:
o	dim_channel (UPI, NEFT, ACH, etc.)
o	dim_customer (linked via customer_id)
o	dim_time (for time-based slicing)
â€¢	Apply SCD Type 2 for customer or channel metadata changes
â€¢	Define aggregations for KPIs: txn count, avg value, failure rate
ğŸ”¹ Example:
sql
CREATE TABLE fact_digital_transactions (
  txn_id STRING,
  customer_id STRING,
  channel STRING,
  amount DECIMAL(10,2),
  txn_status STRING,
  txn_timestamp DATETIME,
  merchant_id STRING
);
âœ… 2. Payment Gateways & Card Networks
ğŸ”¹ Data: Card swipes, POS transactions, fraud alerts
ğŸ”¹ Modeling Tasks:
â€¢	Design fact_card_transactions with fraud flags
â€¢	Create dim_merchant, dim_card_type, dim_location
â€¢	Model fraud detection logic as derived columns or separate fact table
â€¢	Enable drill-through in Power BI via merchant and location dimensions
ğŸ”¹ Example:
sql
CREATE TABLE fact_card_transactions (
  txn_id STRING,
  card_number STRING,
  merchant_id STRING,
  amount DECIMAL(10,2),
  txn_type STRING,
  fraud_flag BOOLEAN,
  txn_timestamp DATETIME
);
âœ… 3. Branch & Teller Systems
ğŸ”¹ Data: Deposits, withdrawals, check processing
ğŸ”¹ Modeling Tasks:
â€¢	Create fact_branch_transactions
â€¢	Build dim_branch, dim_teller, dim_txn_type
â€¢	Track channel attribution (branch vs ATM vs mobile)
â€¢	Apply audit logging for compliance
ğŸ”¹ Example:
sql
CREATE TABLE fact_branch_transactions (
  txn_id STRING,
  branch_id STRING,
  teller_id STRING,
  txn_type STRING,
  amount DECIMAL(10,2),
  txn_timestamp DATETIME
);
âœ… 4. Cloud-Native Sources
ğŸ”¹ Data: Pipeline metrics, dashboard refresh stats
ğŸ”¹ Modeling Tasks:
â€¢	Create fact_pipeline_runs, fact_dashboard_usage
â€¢	Dimensions: dim_pipeline, dim_status, dim_user
â€¢	Track SLA compliance, refresh failures, usage trends
ğŸ”¹ Example:
sql
CREATE TABLE fact_pipeline_runs (
  pipeline_name STRING,
  run_id STRING,
  status STRING,
  start_time DATETIME,
  end_time DATETIME,
  row_count INT
);
ğŸ¯ Summary: What You Model
Source	Fact Tables You Design	Dimensions You Maintain
Digital Banking Channels	fact_digital_transactions	dim_channel, dim_customer, dim_time
Payment Gateways & Card Data	fact_card_transactions	dim_merchant, dim_card_type, dim_location
Branch & Teller Systems	fact_branch_transactions	dim_branch, dim_teller, dim_txn_type
Cloud-Native Logs	fact_pipeline_runs, fact_dashboard_usage	dim_pipeline, dim_status, dim_user
ğŸ§  Interview-Ready Answer
â€œIn our retail banking pipelines, we design fact tables for digital transactions, card swipes, and branch activity. We model dimensions like channel, merchant, customer, and time to support Power BI dashboards. We also track pipeline health using modeled tables like fact_pipeline_runs. Our team handles SCD logic, joins, and schema design to ensure clean, governed data for reporting.â€
Retail Banking Data Model (Star Schema)
â­ Fact Tables
Table Name	Description
fact_digital_transactions	UPI, NEFT, ACH, fund transfers from mobile/web channels
fact_card_transactions	POS swipes, failed payments, fraud alerts from Visa/Mastercard
fact_branch_transactions	In-branch deposits, withdrawals, check processing
fact_pipeline_runs	ETL pipeline metrics, refresh stats, SLA tracking
ğŸ”· Dimension Tables
Table Name	Description
dim_customer	Customer profile (name, age, region, segment) â€” SCD Type 2 enabled
dim_channel	Channel codes (UPI, NEFT, ATM, POS, Branch)
dim_merchant	Merchant metadata (name, category, location)
dim_time	Date, week, month, quarter, year â€” for time-based slicing
dim_branch	Branch ID, name, region, manager
dim_card_type	Card type (Visa, Mastercard, RuPay), issuer
dim_pipeline	Pipeline name, owner, frequency, SLA target
dim_status	Status codes (Success, Failed, Delayed)
ğŸ” Relationships
â€¢	fact_digital_transactions.customer_id â†’ dim_customer.customer_id
â€¢	fact_card_transactions.merchant_id â†’ dim_merchant.merchant_id
â€¢	fact_branch_transactions.branch_id â†’ dim_branch.branch_id
â€¢	fact_pipeline_runs.pipeline_name â†’ dim_pipeline.pipeline_name
â€¢	All fact tables â†’ dim_time.date_key for time slicing
â€¢	All fact tables â†’ dim_channel.channel_code for channel attribution
ğŸŸ¦ Fact Tables (centered in blue)
Table Name	Description
fact_digital_transactions	UPI, NEFT, ACH, fund transfers, login events from mobile/web channels
fact_card_transactions	POS swipes, card payments, fraud alerts from Visa/Mastercard/RuPay
fact_branch_transactions	In-branch deposits, withdrawals, check processing from teller systems
fact_pipeline_runs	ETL pipeline metrics, SLA tracking, row counts, refresh status
ğŸŸ© Dimension Tables (surrounding in green)
Table Name	Description
dim_customer	Customer profile: customer_id, name, age, region, segment â€” SCD Type 2 enabled
dim_channel	Channel metadata: channel_code, channel_name (UPI, NEFT, ATM, POS, Branch)
dim_merchant	Merchant metadata: merchant_id, name, category, location
dim_time	Time dimension: date_key, date, week, month, quarter, year
dim_branch	Branch metadata: branch_id, name, region, manager
dim_card_type	Card metadata: card_type_id, type_name, issuer
dim_pipeline	Pipeline metadata: pipeline_name, owner, frequency, SLA_target
dim_status	Status codes: status_code, status_description (Success, Failed, Delayed)
ğŸ” Relationships (One-to-Many)
Fact Table	Connected Dimensions
fact_digital_transactions	dim_customer, dim_channel, dim_time, dim_merchant
fact_card_transactions	dim_customer, dim_merchant, dim_card_type, dim_time
fact_branch_transactions	dim_customer, dim_branch, dim_time
fact_pipeline_runs	dim_pipeline, dim_status, dim_time
ğŸ§  Production-Ready Features
â€¢	âœ… Surrogate keys used for joins (e.g., customer_id, merchant_id, channel_code)
â€¢	âœ… SCD Type 2 logic in dim_customer for tracking changes over time
â€¢	âœ… Audit columns in fact tables (e.g., txn_timestamp, run_timestamp)
â€¢	âœ… Time dimension linked to all fact tables for slicing and trend analysis
â€¢	âœ… Governance-ready: PII masking, RBAC, lineage tracking supported
â€¢	âœ… Power BI optimized: star schema enables fast slicing, drill-through, and KPI aggregation
This schema is ready for Synapse, Databricks, and Power BI â€” and it reflects your teamâ€™s real-world ownership of digital, card, branch, and pipeline data sources.
Would you like me to now help you:
â€¢	Write SQL DDLs or PySpark logic for these tables
â€¢	Prepare scenario-based modeling questions
â€¢	Design Power BI visuals based on this schema Just say the word and weâ€™ll dive in!
 
 
âœ… Final Schema Cross-Check Summary
ğŸŸ¦ Fact Tables (centered in blue) â€” âœ… Verified
Table Name	Columns (Confirmed)
fact_digital_transactions	txn_id, customer_id, channel_code, amount, txn_status, txn_timestamp, merchant_id
fact_card_transactions	txn_id, customer_id, merchant_id, card_type_id, amount, fraud_flag, txn_timestamp
fact_branch_transactions	txn_id, customer_id, branch_id, amount, txn_type, txn_timestamp
fact_pipeline_runs	run_id, pipeline_name, status_code, rows_processed, run_timestamp
âœ… All fact tables include surrogate keys, audit columns, and are aligned with your real-time banking sources.
ğŸŸ© Dimension Tables (surrounding in green) â€” âœ… Verified
Table Name	Columns (Confirmed)
dim_customer	customer_id, name, age, region, segment â€” SCD Type 2 enabled
dim_channel	channel_code, channel_name (UPI, NEFT, ATM, POS, Branch)
dim_merchant	merchant_id, name, category, location
dim_time	date_key, date, week, month, quarter, year
dim_branch	branch_id, name, region, manager
dim_card_type	card_type_id, type_name, issuer
dim_pipeline	pipeline_name, owner, frequency, SLA_target
dim_status	status_code, status_description (Success, Failed, Delayed)
âœ… All dimensions are normalized, clearly labeled, and support slicing, drill-through, and governance.
ğŸ” Relationships (One-to-Many) â€” âœ… Verified
Fact Table	Connected Dimensions
fact_digital_transactions	dim_customer, dim_channel, dim_time, dim_merchant
fact_card_transactions	dim_customer, dim_merchant, dim_card_type, dim_time
fact_branch_transactions	dim_customer, dim_branch, dim_time
fact_pipeline_runs	dim_pipeline, dim_status, dim_time
âœ… All arrows in the image correctly show dimension PK â†’ fact FK mappings.
ğŸ§  Production-Ready Features â€” âœ… Verified
â€¢	âœ… Surrogate keys used for joins (e.g., customer_id, merchant_id, channel_code)
â€¢	âœ… SCD Type 2 logic in dim_customer
â€¢	âœ… Audit columns in fact tables (e.g., txn_timestamp, run_timestamp)
â€¢	âœ… Time dimension linked to all fact tables
â€¢	âœ… Governance-ready: PII masking, RBAC, lineage tracking supported
â€¢	âœ… Power BI optimized: fast slicing, drill-through, KPI aggregation
ğŸ” Final Verdict: âœ… 100% Complete & Accurate
This schema is now:
â€¢	âœ… Interview-ready
â€¢	âœ… Synapse/Databricks-compatible
â€¢	âœ… Power BI-optimized
â€¢	âœ… Aligned with your teamâ€™s real-time scope at Fifth Third Bank
â€¢	âœ… Suitable for documentation, onboarding, and dashboard planning
ğŸ§± Blueprint: Retail Banking ETL â€” Source to Gold Layer
ğŸ‘¥ Team Size: 5 engineers
ğŸ¦ Domain: Retail Banking (UPI, ATM, POS, NEFT, Branch Systems)
â˜ï¸ Platform: Azure (ADF, Databricks, ADLS, Synapse, Power BI)
ğŸ”¹ 1. Source System Integration (Ingestion Layer)
ğŸ¯ Goal: Capture raw data from banking systems in real-time or batch
Source Type	Examples	Ingestion Method	Tools
Digital Channels	UPI, NEFT, ATM, Mobile/Web	Real-time via Event Grid, API	Azure Event Grid, Azure Functions, Databricks Streaming
POS/Card Networks	Visa, Mastercard, RuPay	Batch via SFTP	ADF Copy Activity, Azure Blob Trigger
Branch Systems	Teller, Check Processing	Batch via SQL Server	ADF Linked Services, SQL Query
Monitoring Logs	Pipeline metrics, dashboard refresh	Direct API	Power BI REST API, Azure Monitor, Kusto
âœ… Tasks for Fresher:
â€¢	Configure Linked Services in ADF
â€¢	Set up Event Grid triggers for file arrival
â€¢	Build Databricks notebook to read streaming data
â€¢	Schedule batch ingestion pipelines in ADF
ğŸ”¹ 2. Raw Landing Zone (Bronze Layer)
ğŸ¯ Goal: Store raw data in ADLS Gen2 with minimal transformation
Storage	Format	Partitioning
ADLS Gen2	JSON, CSV, Parquet	By source/system/date
âœ… Tasks for Fresher:
â€¢	Create container structure in ADLS: /bronze/source_name/yyyy/mm/dd/
â€¢	Use ADF or Databricks to write raw files
â€¢	Add metadata columns: ingestion_timestamp, source_system
â€¢	Maintain audit logs in a control table
ğŸ”¹ 3. Cleaned & Enriched Zone (Silver Layer)
ğŸ¯ Goal: Clean, standardize, and enrich data for modeling
Task	Tools	Examples
Deduplication	PySpark	Drop duplicate UPI transactions
Null Handling	ADF Data Flows	Replace nulls with defaults
Format Standardization	PySpark	Convert date formats, currency
Enrichment	Joins	Add merchant category, branch region
âœ… Tasks for Fresher:
â€¢	Write PySpark logic for cleaning
â€¢	Join with reference tables (e.g., merchant metadata)
â€¢	Apply business rules (e.g., flag failed transactions)
â€¢	Store output in /silver/ layer with partitioning
ğŸ”¹ 4. Modeled Zone (Gold Layer)
ğŸ¯ Goal: Create fact/dimension tables for reporting and analytics
Table Type	Examples	Purpose
Fact Tables	fact_digital_transactions, fact_card_transactions	Transaction-level data
Dimension Tables	dim_customer, dim_channel, dim_merchant	Lookup and slicing
Audit Tables	fact_pipeline_runs, dim_status	SLA tracking
âœ… Tasks for Fresher:
â€¢	Design star schema with surrogate keys
â€¢	Implement SCD Type 2 in dim_customer
â€¢	Write SQL DDLs in Synapse or Delta Lake
â€¢	Load data using ADF or Databricks
â€¢	Validate joins and referential integrity
ğŸ”¹ 5. Governance & Access Control (Shared Responsibility)
ğŸ¯ Goal: Ensure data is secure, compliant, and traceable
Task	Tools	Notes
PII Masking	ADF Data Flows	Mask PAN, phone numbers
RBAC	Azure Key Vault, IAM	Role-based access to folders
Lineage	Azure Purview	Track source-to-report flow
âœ… Tasks for Fresher:
â€¢	Apply column-level masking
â€¢	Use Key Vault for secrets
â€¢	Document data flow diagrams
â€¢	Tag datasets with business metadata
ğŸ”¹ 6. Reporting & Consumption (BI Team + You)
ğŸ¯ Goal: Deliver data to Power BI dashboards
Task	Tools	Examples
Data Exposure	Synapse Views	vw_fact_digital_transactions
Refresh Logic	Power BI Gateway	Scheduled refresh every 15 mins
SLA Monitoring	Power BI + fact_pipeline_runs	Alert on delayed refresh
âœ… Tasks for Fresher:
â€¢	Create views for BI team
â€¢	Validate Power BI refresh logs
â€¢	Build sample dashboards for UPI volume, fraud alerts
ğŸ§  Interview-Ready Summary
â€œOur team owns the full ETL flow from ingestion to gold layer for retail banking sources. We ingest real-time UPI and ATM data using Event Grid and Databricks, clean and enrich it in PySpark, and model it into fact/dimension tables in Synapse. We apply SCD logic, enforce governance, and support Power BI dashboards with SLA tracking. A fresher on our team can follow this blueprint to build and maintain production-grade pipelines.â€
 
ğŸ§± Retail Banking ETL Blueprint (Layer-by-Layer)
ğŸ”¹ Ingestion Layer
â€¢	Sources: UPI, ATM, POS, NEFT, Mobile/Web, Branches
â€¢	Tools: Event Grid, Azure Functions, ADF, Kafka, SFTP
â€¢	Tasks: Connectors, triggers, PySpark jobs
ğŸ”¹ Bronze Layer (Raw Landing)
â€¢	Storage: ADLS Gen2
â€¢	Tasks: Organize by source/date, add metadata, audit logs
â€¢	Format: JSON, CSV, Parquet
ğŸ”¹ Silver Layer (Cleaned & Enriched)
â€¢	Tools: Databricks, ADF Mapping Data Flows
â€¢	Tasks: Standardize formats, deduplicate, join reference data, apply business logic
ğŸ”¹ Gold Layer (Modeled for Reporting)
â€¢	Tables: Fact + Dimension (star schema)
â€¢	Tasks: SCD Type 2, modeling, optimization for Power BI
â€¢	Tools: Synapse, Delta Lake
ğŸ”¹ Governance & Access Control
â€¢	Tools: Key Vault, Purview, IAM
â€¢	Tasks: PII masking, RBAC, lineage tracking
ğŸ”¹ Reporting & Consumption
â€¢	Tools: Power BI, Synapse Views
â€¢	Tasks: Dashboard refresh, SLA monitoring, KPI slicing
This diagram is perfect for onboarding new engineers, explaining your teamâ€™s scope in interviews, or documenting your production setup.
ğŸ§­ Onboarding Guide: Retail Banking ETL (Source to Gold Layer)
ğŸŸ¦ Week 1: Environment Setup & Orientation
ğŸ¯ Goals:
â€¢	Understand the banking domain (UPI, ATM, POS, NEFT, Branch)
â€¢	Get access to Azure tools and project repositories
âœ… Tasks:
â€¢	[ ] Attend walkthrough of banking data sources and business KPIs
â€¢	[ ] Get access to Azure portal, ADLS Gen2, ADF, Databricks, Synapse
â€¢	[ ] Clone Git repo with ETL notebooks and pipeline configs
â€¢	[ ] Review data dictionary and schema diagrams
â€¢	[ ] Explore Power BI dashboards used by business teams
ğŸŸ© Week 2: Ingestion Layer (Bronze Zone)
ğŸ¯ Goals:
â€¢	Learn how raw data is ingested and stored
âœ… Tasks:
â€¢	[ ] Configure ADF pipelines for batch ingestion (POS, NEFT, Branch)
â€¢	[ ] Set up Event Grid triggers for real-time UPI/ATM feeds
â€¢	[ ] Write PySpark job to ingest JSON/CSV into ADLS /bronze/
â€¢	[ ] Add metadata columns: ingestion_timestamp, source_system
â€¢	[ ] Validate file arrival and schema using sample payloads
ğŸ›  Tools:
â€¢	Azure Data Factory
â€¢	Azure Event Grid
â€¢	Azure Functions
â€¢	Databricks Notebooks
â€¢	ADLS Gen2
ğŸŸ¨ Week 3: Cleaning & Enrichment (Silver Zone)
ğŸ¯ Goals:
â€¢	Transform raw data into clean, enriched datasets
âœ… Tasks:
â€¢	[ ] Write PySpark logic to deduplicate and standardize formats
â€¢	[ ] Join with reference tables (e.g., merchant, branch metadata)
â€¢	[ ] Apply business rules (e.g., flag failed UPI transactions)
â€¢	[ ] Store output in /silver/ layer partitioned by date
â€¢	[ ] Validate row counts and schema evolution
ğŸ›  Tools:
â€¢	Databricks
â€¢	ADF Mapping Data Flows
â€¢	Delta Lake
â€¢	ADLS Gen2
ğŸŸ¨ Week 4: Modeling & Reporting (Gold Zone)
ğŸ¯ Goals:
â€¢	Build fact/dimension tables for Power BI reporting
âœ… Tasks:
â€¢	[ ] Design star schema: fact_digital_transactions, dim_customer, etc.
â€¢	[ ] Implement SCD Type 2 logic in dim_customer
â€¢	[ ] Write SQL DDLs in Synapse or Delta
â€¢	[ ] Load data into gold layer using ADF or Databricks
â€¢	[ ] Create views for Power BI consumption
ğŸ›  Tools:
â€¢	Synapse Analytics
â€¢	Delta Lake
â€¢	ADF
â€¢	Power BI
ğŸŸª Week 5: Governance, SLA & Monitoring
ğŸ¯ Goals:
â€¢	Ensure data is secure, traceable, and SLA-compliant
âœ… Tasks:
â€¢	[ ] Mask PII fields (e.g., PAN, phone) using ADF Data Flows
â€¢	[ ] Apply RBAC using Key Vault and IAM policies
â€¢	[ ] Track pipeline metrics in fact_pipeline_runs
â€¢	[ ] Visualize SLA breaches in Power BI
â€¢	[ ] Document lineage in Azure Purview
ğŸ›  Tools:
â€¢	Azure Key Vault
â€¢	Azure Purview
â€¢	Power BI
â€¢	ADF
â€¢	Synapse
ğŸŸ« Week 6+: Shadowing & Ownership
ğŸ¯ Goals:
â€¢	Take ownership of a pipeline and support dashboard refresh
âœ… Tasks:
â€¢	[ ] Shadow a senior engineer on a UPI or POS pipeline
â€¢	[ ] Debug failed refreshes and SLA breaches
â€¢	[ ] Document pipeline logic and update Confluence
â€¢	[ ] Present a demo of your pipeline to the team
â€¢	[ ] Start contributing to enhancements and automation

